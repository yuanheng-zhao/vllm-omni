# Stage config for Ming-flash-omni-2.0
# Stage 0: Thinker (Multimodal understanding + text generation)
# Stage 1a: Image Generator (Text embeddings → PIL image)
# Stage 1b: Talker (Text embeddings → audio waveform)

async_chunk: false
stage_args:
  - stage_id: 0
    stage_type: llm
    runtime:
      devices: "0,1,2,3"
      max_batch_size: 1
    engine_args:
      model_stage: thinker
      model_arch: MingFlashOmniForConditionalGeneration
      worker_type: ar
      scheduler_cls: vllm_omni.core.sched.omni_ar_scheduler.OmniARScheduler
      gpu_memory_utilization: 0.9
      enforce_eager: false
      trust_remote_code: true
      engine_output_type: latent
      distributed_executor_backend: "mp"
      enable_prefix_caching: false
      max_num_batched_tokens: 32768
      tensor_parallel_size: 4  # Use 4 GPUs for MoE model
      hf_config_name: llm_config
    final_output: true  # Can output text directly
    final_output_type: text
    is_comprehension: true
    default_sampling_params:
      temperature: 0.4
      top_p: 0.9
      max_tokens: 2048
      repetition_penalty: 1.05
      seed: 42
      detokenize: true

  # Future Stage 1a: Image Generator (Optional - not yet implemented)
  # - stage_id: 1
  #   stage_type: llm
  #   runtime:
  #     devices: "2"  # Separate GPU for image generation
  #     max_batch_size: 8
  #   engine_args:
  #     model_stage: imagegen
  #     model_arch: MingFlashOmniForConditionalGeneration
  #     worker_type: generation
  #     scheduler_cls: vllm_omni.core.sched.omni_generation_scheduler.OmniGenerationScheduler
  #     gpu_memory_utilization: 0.8
  #     enforce_eager: true
  #     trust_remote_code: true
  #     engine_output_type: image  # Final output: PIL image
  #     max_num_batched_tokens: 1000000
  #   engine_input_source: [0]
  #   custom_process_input_func: vllm_omni.model_executor.stage_input_processors.ming_flash_omni.thinker2imagegen
  #   final_output: true
  #   final_output_type: image
  #   default_sampling_params:
  #     temperature: 0.0
  #     max_tokens: 1

  # Future Stage 1b: Talker/TTS (Optional - not yet implemented)
  # - stage_id: 2
  #   stage_type: llm
  #   runtime:
  #     devices: "3"  # Separate GPU for audio generation
  #     max_batch_size: 8
  #   engine_args:
  #     model_stage: talker
  #     model_arch: MingFlashOmniForConditionalGeneration
  #     worker_type: generation
  #     scheduler_cls: vllm_omni.core.sched.omni_generation_scheduler.OmniGenerationScheduler
  #     gpu_memory_utilization: 0.6
  #     enforce_eager: true
  #     trust_remote_code: true
  #     engine_output_type: audio  # Final output: audio waveform
  #     max_num_batched_tokens: 1000000
  #   engine_input_source: [0]
  #   custom_process_input_func: vllm_omni.model_executor.stage_input_processors.ming_flash_omni.thinker2talker
  #   final_output: true
  #   final_output_type: audio
  #   default_sampling_params:
  #     temperature: 0.0
  #     max_tokens: 1
